#!/bin/bash

# Script para configurar e inicializar o Ollama
# Este script deve ser executado ap√≥s o docker-compose up

echo "ü§ñ Configurando Ollama para o Bi UAI Bot Administrador..."

# Aguardar o servi√ßo ollama estar dispon√≠vel
echo "‚è≥ Aguardando Ollama estar online..."
max_attempts=30
attempts=0

while [ $attempts -lt $max_attempts ]; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "‚úÖ Ollama est√° online!"
        break
    fi
    
    attempts=$((attempts + 1))
    echo "   Tentativa $attempts/$max_attempts..."
    sleep 5
done

if [ $attempts -eq $max_attempts ]; then
    echo "‚ùå Ollama n√£o ficou dispon√≠vel ap√≥s 150 segundos"
    echo "   Verifique se o servi√ßo est√° rodando: docker-compose ps ollama"
    exit 1
fi

# Verificar modelos dispon√≠veis
echo "üìã Verificando modelos dispon√≠veis..."
available_models=$(curl -s http://localhost:11434/api/tags | grep -o '"name":"[^"]*"' | cut -d'"' -f4)

if echo "$available_models" | grep -q "llama3.2:3b"; then
    echo "‚úÖ Modelo llama3.2:3b j√° est√° dispon√≠vel"
else
    echo "üì• Baixando modelo llama3.2:3b (isso pode demorar alguns minutos)..."
    
    # Baixar o modelo
    curl -X POST http://localhost:11434/api/pull \
         -H "Content-Type: application/json" \
         -d '{"name": "llama3.2:3b"}' \
         --progress-bar
    
    if [ $? -eq 0 ]; then
        echo "‚úÖ Modelo llama3.2:3b baixado com sucesso!"
    else
        echo "‚ùå Erro ao baixar o modelo llama3.2:3b"
        exit 1
    fi
fi

# Testar o modelo
echo "üß™ Testando o modelo..."
test_response=$(curl -s -X POST http://localhost:11434/api/generate \
                -H "Content-Type: application/json" \
                -d '{
                    "model": "llama3.2:3b",
                    "prompt": "Ol√°, responda apenas: Teste OK",
                    "stream": false,
                    "options": {
                        "temperature": 0.1,
                        "num_predict": 10
                    }
                }')

if echo "$test_response" | grep -q "response"; then
    echo "‚úÖ Modelo testado com sucesso!"
    echo "   Resposta de teste: $(echo "$test_response" | grep -o '"response":"[^"]*"' | cut -d'"' -f4)"
else
    echo "‚ùå Erro ao testar o modelo"
    echo "   Resposta: $test_response"
    exit 1
fi

# Verificar espa√ßo em disco
echo "üíæ Verificando espa√ßo em disco..."
disk_usage=$(df -h | grep -E "/$|/var" | head -1 | awk '{print $5}' | sed 's/%//')
if [ "$disk_usage" -gt 90 ]; then
    echo "‚ö†Ô∏è  Aten√ß√£o: Uso de disco acima de 90% ($disk_usage%)"
    echo "   Considere liberar espa√ßo para melhor performance"
else
    echo "‚úÖ Espa√ßo em disco OK ($disk_usage% usado)"
fi

# Verificar mem√≥ria
echo "üß† Verificando mem√≥ria dispon√≠vel..."
total_mem=$(free -h | grep "^Mem:" | awk '{print $2}')
available_mem=$(free -h | grep "^Mem:" | awk '{print $7}')
echo "   Mem√≥ria total: $total_mem"
echo "   Mem√≥ria dispon√≠vel: $available_mem"

# Status final
echo ""
echo "üéâ Configura√ß√£o do Ollama conclu√≠da!"
echo ""
echo "üìä Status dos Servi√ßos:"
echo "   ‚Ä¢ Ollama: ‚úÖ Online (http://localhost:11434)"
echo "   ‚Ä¢ Modelo: ‚úÖ llama3.2:3b dispon√≠vel"
echo "   ‚Ä¢ MCP Chatbot: üîÑ Verificando..."

# Verificar se o servi√ßo MCP Chatbot est√° rodando
if curl -s http://localhost:8002/health > /dev/null 2>&1; then
    echo "   ‚Ä¢ MCP Chatbot: ‚úÖ Online (http://localhost:8002)"
else
    echo "   ‚Ä¢ MCP Chatbot: ‚è≥ Aguardando inicializa√ß√£o..."
fi

echo ""
echo "üöÄ Bi UAI Bot Administrador est√° pronto para uso!"
echo ""
echo "üí° Dicas:"
echo "   ‚Ä¢ Acesse o sistema BIUAI e clique no √≠cone do bot no canto inferior direito"
echo "   ‚Ä¢ Admins podem gerenciar o bot em: /admin/chatbot"
echo "   ‚Ä¢ Para logs do chatbot: docker-compose logs -f mcp-chatbot-service"
echo "   ‚Ä¢ Para logs do Ollama: docker-compose logs -f ollama"
echo "" 